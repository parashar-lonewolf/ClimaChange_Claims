{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing datset for chunk tag sequence prediction \n",
    "\n",
    "* Collecting the two ConLL datasets and combining them\n",
    "* All annotation tags available as Lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read\n",
    "anno_text_ank = open(\"Data/ConLLformat_annotator_ank.txt\").read()\n",
    "anno_text_ian = open(\"Data/ConLLformat_annotator_ian.txt\").read()\n",
    "\n",
    "## Combine\n",
    "anno_data = anno_text_ian.split(\"\\n\")\n",
    "anno_data += anno_text_ank.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The annotation tags\n",
    "tag = ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '``']\n",
    "ner_tags = ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
    "parser = ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n",
    "actor_claim_tag = ['O','B-ACT','I-ACT','B-CLAIM','I-CLAIM']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions to convert data into dataset:\n",
    "* Format: dataset = [[(w1,t11,t12..)],[(w2,t21,t22..)]] \n",
    "* where w = word, t = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Raw annotated data into dataset for training/testing\n",
    "def dataset_creator(split_data,actor_claim_tag):\n",
    "    dataset = []\n",
    "    i = 0\n",
    "    newLine = []\n",
    "    prev_label = 'O'\n",
    "    for line in split_data[1:]:\n",
    "        l = tuple(line.split(\"\\t\"))\n",
    "        if(len(l)==8):\n",
    "            dataset.append(newLine)\n",
    "            newLine = []\n",
    "            if(len(l[1:-2])==4):\n",
    "                temp_l = list(l[1:-2]).append(prev_label)\n",
    "            else:\n",
    "                temp_l = list(l[1:-2])\n",
    "                prev_label = temp_l[-1]\n",
    "        else:\n",
    "            if(len(l[1:-1])<4):\n",
    "                newLine = []\n",
    "            elif(len(l[1:-1])==4):\n",
    "                temp_l = list(l[1:-1])\n",
    "                temp_l.append(prev_label)                \n",
    "            else:\n",
    "                temp_l = list(l[1:-1])\n",
    "                prev_label = temp_l[-1]\n",
    "                \n",
    "        if(temp_l[-1] not in actor_claim_tag):\n",
    "            temp_l[-1]='O'\n",
    "        \n",
    "        newLine.append(tuple(temp_l))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset is created\n",
    "data = dataset_creator(anno_data,actor_claim_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature extraction\n",
    "* Will create and assign features for CRF model to train with using all the other tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converts each word to feature consumable by a CRF model\n",
    "def word_make_features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    tag = doc[i][1]\n",
    "    ner_tags = doc[i][2]\n",
    "    parser= doc[i][3]\n",
    "    try:\n",
    "        actor_claim_tag = doc[i][4]\n",
    "    except:\n",
    "        print(doc[i])\n",
    "\n",
    "    # Common features for all words\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word.tag =' + tag,\n",
    "        'word.ner_tags=' + ner_tags,\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'word.parser=' + parser\n",
    "    ]\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a document\n",
    "    if i > 0:\n",
    "        word1 = doc[i-1][0]\n",
    "        tag1 = doc[i-1][1]\n",
    "        ner_tags1 = doc[i-1][2]\n",
    "        parser1 = doc[i-1][3]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.tag =' + tag1,\n",
    "            '-1:word.ner_tags=' + ner_tags1,\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:parser=' + parser1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "        word1 = doc[i+1][0]\n",
    "        tag1 = doc[i+1][1]\n",
    "        ner_tags1 = doc[i+1][2]\n",
    "        parser1 = doc[i+1][3]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.tag =' + tag1,\n",
    "            '-1:word.ner_tags=' + ner_tags1,\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:parser=' + parser1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Combine the featire creation to features per sentence consumable by the model\n",
    "## A function for extracting features in documents\n",
    "def features_extraction(doc):\n",
    "    return [word_make_features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "## A function fo generating the list of labels for each document\n",
    "def extract_label_per_sent(doc):\n",
    "    Label = []\n",
    "    for (token,tag, ner_tags,parser,label) in doc:\n",
    "        Label.append(label)\n",
    "    return(Label)\n",
    "\n",
    "## Collecting all features and labels\n",
    "all_features = [features_extraction(doc) for doc in data]\n",
    "all_labels = [extract_label_per_sent(doc) for doc in data]\n",
    "\n",
    "## The dataset fully prepared\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#####  Training a model\n",
    "* CRF model with c1 and c2 penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## In pycrfsuite, A CRF model in can be trained by first creating a \n",
    "## trainer, and then submit the training data and corresponding labels\n",
    "## to the trainer. After that, set the parameters and call train() to \n",
    "## start the training process\n",
    "## CRFSuite: http://www.chokkan.org/software/crfsuite/manual.html#idp8849114176\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=True)\n",
    "\n",
    "# initiate training data to the trainer\n",
    "for x_features, y_labels in zip(X_train, y_train):\n",
    "    trainer.append(x_features, y_labels)\n",
    "\n",
    "# Set the parameters of the model\n",
    "trainer.set_params({\n",
    "    'c1': 0.85,   # coefficient for L1 penalty\n",
    "    'c2': 0.0094,  # coefficient for L2 penalty\n",
    "    # maximum number of iterations\n",
    "    'max_iterations': 500,\n",
    "\n",
    "    # whether to include transitions that\n",
    "    # are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Provide a file name as a parameter to the train function, such that\n",
    "# the model will be saved to the file when training is finished\n",
    "# trainer.train('Pickles/crf_KyotoData_version_1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Testing and Evaluating the model\n",
    "* checking the poutput of tagger for random test inputs adn create an evaluation matrix for the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and \t(O)\n",
      "i \t(O)\n",
      "-- \t(O)\n",
      "i \t(O)\n",
      "feel \t(O)\n",
      "that \t(O)\n",
      "for \t(O)\n",
      "a \t(O)\n",
      "simple \t(O)\n",
      "reason \t(O)\n",
      ": \t(O)\n",
      "  \t(O)\n"
     ]
    }
   ],
   "source": [
    "## Once the tagger is done and dusted we test it on the test data\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('Pickles/crf_KyotoData_version_1.model')\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "# Let's take a look at a random sample in the testing set\n",
    "i = 13\n",
    "for x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n",
    "    print(\"%s \\t(%s)\" % (y, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 5, 'B-ACT': 4, 'I-ACT': 3, 'B-CLAIM': 2, 'I-CLAIM': 1}\n"
     ]
    }
   ],
   "source": [
    "# dataset for evaluation\n",
    "labels = {}\n",
    "line_count = len(actor_claim_tag)\n",
    "for i in actor_claim_tag:\n",
    "    labels[i]=line_count\n",
    "    line_count-=1\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[tag] for row in y_pred for tag in row])\n",
    "truths = np.array([labels[tag] for row in y_test for tag in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.28      0.11      0.16      1244\n",
      "       B-ACT       0.23      0.05      0.08       101\n",
      "       I-ACT       0.59      0.17      0.27       128\n",
      "     B-CLAIM       0.45      0.12      0.18        78\n",
      "     I-CLAIM       0.88      0.97      0.92     10459\n",
      "\n",
      "    accuracy                           0.86     12010\n",
      "   macro avg       0.49      0.28      0.32     12010\n",
      "weighted avg       0.81      0.86      0.82     12010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names=actor_claim_tag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climachange_env",
   "language": "python",
   "name": "climachange_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
