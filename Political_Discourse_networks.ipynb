{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "import jgraph\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib._color_data as mcd\n",
    "import matplotlib.patches as mpatch\n",
    "\n",
    "from IPython.display import Markdown\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Political Discourse Network  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Claim clusters and cleaned actor list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Pickles/clustered_claims_final3.pkl\", 'rb') as f:\n",
    "    ##                                  max_df,min_df,ngram_range\n",
    "    ## claim_list,top6_cluster_features,0.2,   0.02,  (1,6)\n",
    "    arr_text = pickle.load(f)\n",
    "    claim_text = arr_text[0]\n",
    "    claim_names = arr_text[1]\n",
    "\n",
    "# with open(\"clean_actors_PolDis_v2.pkl\", 'rb') as f:\n",
    "#     actor = pickle.load(f)\n",
    "with open(\"Pickles/supp_opp_final3.pkl\", 'rb') as f:\n",
    "    supp_opp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Timestamp list for time-lapse graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gather trimestamps\n",
    "claim_text_ian = open(\"Save/Claims_ian.txt\").read()\n",
    "claims_split_ian = claim_text_ian.split(\"\\n\\n#\") \n",
    "# actor = []\n",
    "# claims = []\n",
    "timestamps_ian = []\n",
    "count = 0\n",
    "for c in claims_split_ian:\n",
    "    txt = c.split(\"DATED: \")[-1]\n",
    "    txt1 = txt.split(\"/\")[0]+\"/\"+txt.split(\"/\")[1]\n",
    "#     txt = txt.replace(txt1,\"\")\n",
    "    timestamps_ian.append(txt1)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gather trimestamps\n",
    "claim_text_ank = open(\"Save/Claims_ank.txt\").read()\n",
    "claims_split_ank = claim_text_ank.split(\"\\n\\n#\") \n",
    "# actor = []\n",
    "# claims = []\n",
    "timestamps_ank = []\n",
    "count = 0\n",
    "for c in claims_split_ank:\n",
    "    txt = c.split(\"DATED: \")[-1]\n",
    "    txt1 = txt.split(\"/\")[0]+\"/\"+txt.split(\"/\")[1]\n",
    "#     txt = txt.replace(txt1,\"\")\n",
    "    timestamps_ank.append(txt1)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extracting all dates list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# t = datetime.strptime(date_time_str, '%d/%m')\n",
    "\n",
    "\n",
    "filename = open(\"4mFullData_Kyoto_filename_list.txt\").read()\n",
    "filename_split =filename.split()\n",
    "dates = []\n",
    "for f in filename_split:\n",
    "    f = f.split(\"/\")[1] \n",
    "    txt = f.split(\"_\")[0:3][:-1]\n",
    "    t = (\"/\").join(txt)\n",
    "    dates.append(t)\n",
    "#     print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_ian_corr = []\n",
    "for i in timestamps_ian:\n",
    "    count = 0\n",
    "    for j in range(0,len(dates)):\n",
    "        if i == dates[j]:\n",
    "            timestamps_ian_corr.append(dates[j+452])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the actors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = []\n",
    "for i in claim_text['actor']:\n",
    "    i = i.replace(\"'\",\"\")\n",
    "    actor.append(i)\n",
    "claim_text['actor'] = actor\n",
    "# print(claim_text['actor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claims_split = claims_split_ank + claims_text_ian\n",
    "timestamps = timestamps_ank + timestamps_ian_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## special curated list\n",
    "act_stopwords = ['mr.','dr.','i','the','a','we','of','and','other ','at','his','number','some',\n",
    "             'officials','department','programs','agency','coalition','leaders','organizations','expert'\n",
    "            'groups','powerful','organization','institute','trade', 'industry','advisers','laboratories',\n",
    "             'fund','policy','university','panel','treaty','union','view','dominant','senator']\n",
    "tricky_words = ['president','administrations','countries']\n",
    "\n",
    "## actor group clustering\n",
    "act_count = 0\n",
    "act_cluster = [-1]*len(actor)\n",
    "cluster_no = 0\n",
    "\n",
    "## regex-substituter for actor stopwords\n",
    "replace_stopwords = re.compile('|'.join(map(re.escape, act_stopwords)))\n",
    "act_cluster_keys = []\n",
    "## searching for word by word exactness of actors\n",
    "while True: ######################################################### SUPER_LOOP START\n",
    "    ## stop words replaced, so checking not necessary\n",
    "    a1 = (replace_stopwords.sub(\"\",actor[act_count].lower())).split()       # [actor-1] to be checked with....\n",
    "    sub_act_count = 0\n",
    "    ## every super-loop restarts [actor-1] as a NEW CLUSTER\n",
    "    act_cluster[act_count]=cluster_no\n",
    "    \n",
    "    ## looping through every actor till a cluster is formed\n",
    "    while True:  ##------------------------------------------## sub_LOOP START\n",
    "        ## stop words replaced, so checking not necessary\n",
    "        a2 = (replace_stopwords.sub(\"\",actor[sub_act_count].lower())).split()   # ...[actor-2] for semantic overlap\n",
    "\n",
    "        ## proceeding only if [actor-2] is NOT CLUSTERED\n",
    "        if(act_cluster[sub_act_count]==-1):                \n",
    "                ## adding actors to clusters based on word exactness\n",
    "                \n",
    "                ## need two matches minimum per actor for actors with tricky words\n",
    "                a_match = 1+ int(any(i in a1 for i in tricky_words) or any(i in a2 for i in tricky_words))\n",
    "                                \n",
    "                ## 1. checking if they are an exact match\n",
    "                if(\"\".join(a1) == \"\".join(a2)):         \n",
    "                    act_cluster[sub_act_count] = cluster_no\n",
    "\n",
    "                ## 2. checking for partial match except 'stopwords'\n",
    "                else:\n",
    "                    ## looping actor-1 with actor-2 for partial match\n",
    "                    for i in a1:   #-----------------------------------------## actor-1 START\n",
    "                        for j in a2:         #-----------------------------------------## actor-2 START\n",
    "                            ## word by word exactness\n",
    "                            if(i == j):\n",
    "                                a_match-=1\n",
    "                            ##baby-loop-1--------------------------------------## actor-2 END        \n",
    "                        ##baby-loop-1--------------------------------------## actor-1 END\n",
    "                        if(a_match<=0):\n",
    "                            act_cluster[sub_act_count] = cluster_no\n",
    "                            break\n",
    "                                \n",
    "        ## sub-loop count iterator ---------------------------## sub_LOOP END\n",
    "        sub_act_count+=1\n",
    "        ## sub-loop maker and breaker\n",
    "        try:\n",
    "            temp = actor[sub_act_count]\n",
    "        except:\n",
    "            break            \n",
    "        while act_cluster[sub_act_count]!=-1:\n",
    "            sub_act_count+=1\n",
    "            if(sub_act_count==(len(actor)-1)):\n",
    "                break\n",
    "            \n",
    "\n",
    "    act_cluster_keys.append(actor[act_count])        \n",
    "    ## super-loop count iterator##################################### SUPER_LOOP END\n",
    "    act_count+=1\n",
    "    ## super-loop maker and breaker\n",
    "    try:\n",
    "        temp = actor[act_count]\n",
    "    except:\n",
    "        break\n",
    "    while act_cluster[act_count]!=-1:\n",
    "        act_count+=1\n",
    "        if(act_count==(len(actor)-1)):\n",
    "            break\n",
    "    ## at every super loop, one cluster is formed\n",
    "    cluster_no+=1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the colour pallete for the network\n",
    "##          dark    -  colours     \n",
    "claim_color_pallete = [0x490357 ,0x022055 ,0x194B00 ,0x410200 ,0x512402  ]\n",
    "##                     black     increment\n",
    "actor_color_pallete = [0xaaaaaa ,0x010a71  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['actor', 'claims', 'cluster'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## kMeans cluster keys\n",
    "claim_text.keys()\n",
    "# act_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1995/01': 10, '1997/01': 1, '1997/02': 4, '1997/03': 11, '1997/06': 84, '1997/07': 23, '1997/08': 26, '1997/09': 36, '1997/10': 91, '2002/02': 2, '2002/03': 51, '2002/04': 45, '2002/06': 150, '2002/08': 16}\n"
     ]
    }
   ],
   "source": [
    "## cluster counts\n",
    "# act_cluster_counts = {str(i):act_cluster.count(i) for i in act_cluster}\n",
    "# print(len(act_cluster_counts))\n",
    "# print()\n",
    "# claim_cluster_counts = {str(i):claim_text['cluster'].count(i) for i in claim_text['cluster']}\n",
    "# print(len(claim_cluster_counts))\n",
    "timestamps_counts = {str(i):timestamps.count(i) for i in timestamps}\n",
    "print(timestamps_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1995/01', '1997/01', '1997/02', '1997/03', '1997/06', '1997/07', '1997/08', '1997/09', '1997/10', '2002/02', '2002/03', '2002/04', '2002/06', '2002/08'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps_counts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## discourse newtork graph\n",
    "pol_dis_nw     =  {\n",
    "                    'nodes':{},\n",
    "                    'edges':{}\n",
    "                    }\n",
    "edge_subdict    = {'source': '', 'target': '', 'size': 0, 'color':0x000000}\n",
    "node_subsubdict = {'color': 0x000000, 'size': 0.0}\n",
    "claim_text['supp_opp'] = supp_opp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Political Discourse graph of all actor and claim clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for making the actor claim discourse network\n",
    "def act_claim_graph(claim_text,act_cluster,actor,supp_opp,init,end):\n",
    "    ## main vars\n",
    "    act_cluster = act_cluster[init:end]\n",
    "    actor = actor[init:end]\n",
    "    cluster = claim_text['cluster'][init:end]\n",
    "    supp_opp = supp_opp[init:end]\n",
    "    \n",
    "    ## cluster counts\n",
    "    act_cluster_counts = {str(i):act_cluster.count(i) for i in act_cluster}\n",
    "    claim_cluster_counts = {str(i):cluster.count(i) for i in cluster}\n",
    "\n",
    "    col_dict = mcd.CSS4_COLORS\n",
    "    col_keys = list(col_dict.keys())\n",
    "#     print(claim_color_pallete)\n",
    "    ## -1. timewise-graph creator\n",
    "\n",
    "    ## 0. the jgraph \n",
    "    pol_dis_nw     =  {\n",
    "                    'nodes':{},\n",
    "                    'edges':{}\n",
    "                    }\n",
    "    ## 1. edge propertoes\n",
    "    edge_size = {'+':1.5,'-':0.5}\n",
    "    edge_col = {'+':0xFFAF03,'-':0xaaaaaa}\n",
    "    \n",
    "    ## 2. nodes\n",
    "    ## a. actor nodes\n",
    "    nodes = {}\n",
    "    node_colours = {}\n",
    "    \n",
    "    act_col = actor_color_pallete[0]\n",
    "    actXsize = (1/max(act_cluster_counts.values()))*1 \n",
    "    a_count = 147\n",
    "#     print(act_cluster_counts)\n",
    "    \n",
    "    for key in act_cluster_counts.keys():\n",
    "        i = act_cluster_counts[key]\n",
    "        i = 0.5+(actXsize*i)                        ## actor node size range(0.5,1.5)\n",
    "        ## storing node clour for index\n",
    "#         print(key)\n",
    "        node_colours[key] = col_dict[col_keys[a_count]]\n",
    "        hex_int = int(node_colours[key].replace(\"#\",\"0x\"), 16)\n",
    "        new_int = hex_int + 0x200\n",
    "        sub_node = {'color': hex(new_int),'size':i}\n",
    "        nodes[key] = sub_node\n",
    "#         act_col+=actor_color_pallete[1]\n",
    "        a_count-=5\n",
    "    #     break\n",
    "    ## b. claim nodes\n",
    "    # for colour, 'claim_color_pallete'\n",
    "    claimXsize = (1/max(claim_cluster_counts.values()))*2.5\n",
    "    c_count = 17 \n",
    "    node_colours[\"ACTORS\"] = '#FFFFFF'\n",
    "    for key in claim_cluster_counts.keys():\n",
    "        i = claim_cluster_counts[key]\n",
    "        i = 1+(claimXsize*i)                           ## claim node size range(1,3.5)\n",
    "        ## storing node clour for index\n",
    "#         node_colours[] = \"#\"+str(hex(claim_color_pallete[int(key)]))\n",
    "        node_colours['C'+key] = col_dict[col_keys[c_count]]\n",
    "        hex_int = int(node_colours['C'+key].replace(\"#\",\"0x\"), 16)\n",
    "        new_int = hex_int + 0x200\n",
    "        sub_node = {'color': hex(new_int),'size':i}\n",
    "        nodes['C'+key] = sub_node\n",
    "#         act_col+=actor_color_pallete[1]\n",
    "        c_count+=27\n",
    "    #     break\n",
    "    node_colours[\"CLAIMS\"] = '#FFFFFF'\n",
    "    pol_dis_nw['nodes'] = nodes\n",
    "    \n",
    "    \n",
    "    ## 3. edges\n",
    "    ## a. actor nodes\n",
    "    edges = []\n",
    "    for i in range(0,len(actor)):\n",
    "        source = act_cluster[i]\n",
    "        e_size = edge_size[supp_opp[i]]\n",
    "        e_col = edge_col[supp_opp[i]]\n",
    "        sub_edge = {'source': str(source), 'target': 'C'+str(cluster[i]), 'size': 1, 'color': e_col}\n",
    "        edges.append(sub_edge)\n",
    "    #     break\n",
    "    pol_dis_nw['edges'] = edges\n",
    "    \n",
    "    # import json\n",
    "    # print( json.dumps(pol_dis_nw))\n",
    "    # edge_subdict\n",
    "    return(pol_dis_nw,node_colours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Full Graph of entire network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# jgraph.draw(act_claim_graph(claim_text,act_cluster,actor,supp_opp,0,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Month-wise timelapse of actor-claim network  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'s\", 'administration', 'approaches', 'country', 'environmental', 'nations'],\n",
       " ['climate', 'change', 'climate', 'scientific', 'observed', 'earth'],\n",
       " ['kyoto', 'protocol', 'kyoto', 'treaty', 'kyoto', 'according'],\n",
       " ['emissions', 'greenhouse', 'reduce', 'gases', 'cuts', 'carbon'],\n",
       " ['warming', 'global', 'global', 'problem', 'threat', 'environment']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(claim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "## Cluster Representatives:\n",
    "    # claim_names\n",
    "    # act_cluster_keys\n",
    "#############################\n",
    "\n",
    "# key =  in list(timestamps_counts.keys())[2:]:\n",
    "def graph_per_time(claim_text,act_cluster,actor,key,init,end):\n",
    "#     end += (timestamps_counts[key])\n",
    "    nw, col_ix = act_claim_graph(claim_text,act_cluster,actor,supp_opp,init,end)\n",
    "    index = list(col_ix.keys())\n",
    "    ## date printing\n",
    "#     print(index)\n",
    "    display (Markdown('<span style=\"color:#000000\">'+key+'</span>'))\n",
    "    ########################################################\n",
    "    overlap = {name for name in mcd.CSS4_COLORS\n",
    "               if \"xkcd:\" + name in mcd.XKCD_COLORS}\n",
    "    \n",
    "    fig = plt.figure(figsize=[3.8, 6])\n",
    "    AX = fig.add_axes([0, 0, 1, 1])\n",
    "#     index = \"Stuff\"\n",
    "    i = 0\n",
    "    for j, n in enumerate(sorted(overlap, reverse=True)):\n",
    "        weight = \"bold\"\n",
    "        try:\n",
    "            kolor = col_ix[index[i]]\n",
    "            key  = index[i]\n",
    "        except:\n",
    "            break\n",
    "        cn = kolor\n",
    "#         print(cn)\n",
    "        r2 = mpatch.Rectangle((0, j), 1, 1, color=cn)\n",
    "        \n",
    "        if(key!=\"ACTORS\" and key!=\"CLAIMS\" ):\n",
    "            if(key[0]=='C'):\n",
    "#                 print(key)\n",
    "                txt = AX.text(2, j+.5, key+\": \"+\"-\".join(claim_names[int(key[1:])]), va='center', fontsize=8,\n",
    "                      weight=weight)\n",
    "            else:\n",
    "                txt = AX.text(2, j+.5, key+\": \"+act_cluster_keys[int(key)], va='center', fontsize=8,\n",
    "                      weight=weight)\n",
    "        else:\n",
    "            txt = AX.text(2, j+.5, key, va='top', fontsize=10,\n",
    "                      weight=weight)\n",
    "    \n",
    "        AX.add_patch(r2)\n",
    "        AX.axhline(j, color=cn)\n",
    "        i+=1\n",
    "            \n",
    "    AX.text(0.5, 1.5, ' ', ha='center', va='center')\n",
    "    AX.set_xlim(0.5, 3)\n",
    "    AX.set_ylim(0, j + 2)\n",
    "    AX.axis('off')\n",
    "    \n",
    "    return(nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1995/01': 10,\n",
       " '1997/01': 1,\n",
       " '1997/02': 4,\n",
       " '1997/03': 11,\n",
       " '1997/06': 84,\n",
       " '1997/07': 23,\n",
       " '1997/08': 26,\n",
       " '1997/09': 36,\n",
       " '1997/10': 91,\n",
       " '2002/02': 2,\n",
       " '2002/03': 51,\n",
       " '2002/04': 45,\n",
       " '2002/06': 150,\n",
       " '2002/08': 16}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = 0\n",
    "end  = -1\n",
    "n = 7\n",
    "for k in list(timestamps_counts.keys())[n:]:\n",
    "    init = sum(list(timestamps_counts.values())[:n])\n",
    "    end = sum(list(timestamps_counts.values())[:n+1])\n",
    "#     print(timestamps_counts[k])\n",
    "#     print(type(k))\n",
    "    g = graph_per_time(claim_text,act_cluster,actor,k,init,end)\n",
    "    jgraph.draw(g)\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Poltical Discourse graph 1](https://cdn.mathpix.com/snip/images/O2EKLh4k8jWGBAfnmEiNS5UrwBZmQ6Eaas2nF3WDSUc.original.fullsize.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climachange_env",
   "language": "python",
   "name": "climachange_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
