{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pycrfsuite\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing datset for chunk tag sequence prediction \n",
    "\n",
    "* Collecting the two ConLL datasets and combining them\n",
    "* All annotation tags available as Lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = \"Data/\"\n",
    "savefolder = \"Save/\"\n",
    "pickle_folder = \"Pickles/\"\n",
    "\n",
    "# Read\n",
    "anno_text_ank = open(savefolder+\"ConLLformat_annotator_ank.txt\").read()\n",
    "anno_text_ian = open(savefolder+\"ConLLformat_annotator_ian.txt\").read()\n",
    "## Reading the claims cluster to include it in the tags\n",
    "with open(pickle_folder+\"clustered_claims_final3.pkl\", 'rb') as f:\n",
    "    claim_text = pickle.load(f)[0]\n",
    "\n",
    "## Combine\n",
    "anno_data = anno_text_ank.split(\"\\n\") + anno_text_ian.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(claim_text['claims'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The annotation tags\n",
    "tag = ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '``']\n",
    "ner_tags = ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
    "parser = ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp']\n",
    "actor_claim_tag = ['O','B-ACT','I-ACT','B-CLAIM','I-CLAIM']\n",
    "new_actor_claim_tag = ['O','B-ACT','I-ACT','B-CLAIM-0','I-CLAIM-0','B-CLAIM-1','I-CLAIM-1','B-CLAIM-2','I-CLAIM-2','B-CLAIM-3','I-CLAIM-3','B-CLAIM-4','I-CLAIM-4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Functions to convert data into dataset:\n",
    "* Format: dataset = [[(w1,t11,t12..)],[(w2,t21,t22..)]] \n",
    "* where w = word, t = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Raw annotated data into dataset for training/testing\n",
    "def clus_dataset_creator(split_data,actor_claim_tag,no_of_claims):\n",
    "    sup_op_cnt = []\n",
    "    dataset = []\n",
    "    claim_count = 0\n",
    "\n",
    "\n",
    "    the_end = False    ## the function end bool\n",
    "    i = 0              ## line_no-1\n",
    "    while not the_end:\n",
    "            \n",
    "        linebreak = False   ## for reading per_line of ACTUAL sentences\n",
    "        line_arr = []\n",
    "        found_claim = False\n",
    "        \n",
    "        newLine = []\n",
    "        \n",
    "        while not linebreak:\n",
    "            ## feature error handling\n",
    "            try:\n",
    "                per_tag = split_data[i].split(\"\\t\")\n",
    "                xoxo = per_tag[3]\n",
    "            except:\n",
    "                if(claim_count>=(no_of_claims-1)):\n",
    "                    the_end = True\n",
    "                i+=1\n",
    "                linebreak = True\n",
    "                break\n",
    "            ## features corrections\n",
    "            if(len(per_tag)<7):    \n",
    "                per_tag = per_tag[:-1]+['O','']    \n",
    "            #######################################\n",
    "            ## 1. appending to newLine       START\n",
    "            if(len(line_arr)==0):\n",
    "                line_arr.append(per_tag[0])\n",
    "                newLine.append(per_tag[1:7])\n",
    "                i+=1\n",
    "            else:\n",
    "                if(int(per_tag[0]) >= int(line_arr[-1])):\n",
    "                    line_arr.append(per_tag[0])\n",
    "                    newLine.append(per_tag[1:7])\n",
    "                    i+=1\n",
    "                else:\n",
    "                    linebreak = True\n",
    "            ## 1. appending to newLine          END\n",
    "        ##############################################################\n",
    "        modLine = []\n",
    "        for j in newLine:\n",
    "            ## 2. CLAIM hunting       START\n",
    "            if(j[4] not in actor_claim_tag):   ## a. if not in act_claim_tag, use 'O'\n",
    "                j[4] = 'O'\n",
    "            elif(j[4][2:]=='CLAIM'):           ## b. if a CLAIM tag\n",
    "                if(j[4][0] == 'B' and not found_claim):\n",
    "                    sup_op_cnt.append(j[5])\n",
    "                    found_claim = True\n",
    "                    \n",
    "                try:\n",
    "                    j[4] = j[4] + \"-\" + str(claim_text['cluster'][claim_count])\n",
    "                except:\n",
    "                    break\n",
    "            modLine.append(tuple(j[:-1]))        \n",
    "            ## 2. CLAIM hunting          END\n",
    "            ##############################################################\n",
    "\n",
    "        if(found_claim):   # increment only if atleast one claim is found\n",
    "            claim_count+=1\n",
    "            \n",
    "        dataset.append(modLine)\n",
    "    return dataset,sup_op_cnt\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Raw annotated data into dataset for training/testing\n",
    "def noclus_dataset_creator(split_data,actor_claim_tag):\n",
    "    sup_op_cnt = []\n",
    "    dataset = []\n",
    "    claim_count = 0\n",
    "\n",
    "\n",
    "    the_end = False    ## the function end bool\n",
    "    i = 0              ## line_no-1\n",
    "    while not the_end:\n",
    "            \n",
    "        linebreak = False   ## for reading per_line of ACTUAL sentences\n",
    "        line_arr = []\n",
    "\n",
    "        \n",
    "        newLine = []\n",
    "        \n",
    "        while not linebreak:\n",
    "            ## feature error handling\n",
    "            try:\n",
    "                per_tag = split_data[i].split(\"\\t\")\n",
    "                xoxo = per_tag[3]\n",
    "            except:\n",
    "                if(i>=(len(split_data)-1)):\n",
    "                    the_end = True\n",
    "                i+=1\n",
    "                linebreak = True\n",
    "                break\n",
    "            ## features corrections\n",
    "            if(len(per_tag)<7):    \n",
    "                per_tag = per_tag[:-1]+['O','']    \n",
    "            #######################################\n",
    "            ## Appending to newLine       START\n",
    "            if(len(line_arr)==0):\n",
    "                line_arr.append(per_tag[0])\n",
    "                newLine.append(per_tag[1:6])\n",
    "                i+=1\n",
    "            else:\n",
    "                if(int(per_tag[0]) >= int(line_arr[-1])):\n",
    "                    line_arr.append(per_tag[0])\n",
    "                    newLine.append(per_tag[1:6])\n",
    "                    i+=1\n",
    "                else:\n",
    "                    linebreak = True\n",
    "            ## Appending to newLine          END\n",
    "            #######################################\n",
    "            \n",
    "        dataset.append(newLine)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## dataset is created\n",
    "no_of_claims = len(claim_text['cluster'])\n",
    "clus_data , sup_opp = clus_dataset_creator(anno_data,actor_claim_tag,no_of_claims)\n",
    "noclus_data = noclus_dataset_creator(anno_data,actor_claim_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(pickle_folder+\"supp_opp_final3.pkl\", 'wb') as f:\n",
    "#     pickle.dump(sup_opp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature extraction\n",
    "* Will create and assign features for CRF model to train with using all the other tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converts each word to feature consumable by a CRF model\n",
    "def word_make_features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    tag = doc[i][1]\n",
    "    ner_tags = doc[i][2]\n",
    "    parser= doc[i][3]\n",
    "    try:\n",
    "        actor_claim_tag = doc[i][4]\n",
    "    except:\n",
    "        print(doc[i])\n",
    "\n",
    "    # Common features for all words\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word.tag =' + tag,\n",
    "        'word.ner_tags=' + ner_tags,\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'word.parser=' + parser\n",
    "    ]\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a document\n",
    "    if i > 0:\n",
    "        word1 = doc[i-1][0]\n",
    "        tag1 = doc[i-1][1]\n",
    "        ner_tags1 = doc[i-1][2]\n",
    "        parser1 = doc[i-1][3]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.tag =' + tag1,\n",
    "            '-1:word.ner_tags=' + ner_tags1,\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:parser=' + parser1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features.append('BOS')\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "        word1 = doc[i+1][0]\n",
    "        tag1 = doc[i+1][1]\n",
    "        ner_tags1 = doc[i+1][2]\n",
    "        parser1 = doc[i+1][3]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.tag =' + tag1,\n",
    "            '-1:word.ner_tags=' + ner_tags1,\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:parser=' + parser1\n",
    "        ])\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features.append('EOS')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Combine the featire creation to features per sentence consumable by the model\n",
    "## A function for extracting features in documents\n",
    "def features_extraction(doc):\n",
    "    return [word_make_features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "## A function fo generating the list of labels for each document\n",
    "def extract_label_per_sent(doc):\n",
    "    Label = []\n",
    "    for (token,tag, ner_tags,parser,label) in doc:\n",
    "        Label.append(label)\n",
    "    return(Label)\n",
    "\n",
    "def model_data(data,test_split):\n",
    "    ## Collecting all features and labels\n",
    "    all_features = [features_extraction(doc) for doc in data]\n",
    "    all_labels = [extract_label_per_sent(doc) for doc in data]\n",
    "\n",
    "    ## The dataset fully prepared\n",
    "    return(train_test_split(all_features, all_labels, test_size=test_split))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Clustered data and unclustered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cX_train, cX_test, cy_train, cy_test =  model_data(clus_data,0.30)\n",
    "# X_train, X_test, y_train, y_test =  model_data(noclus_data,0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#####  Training a model\n",
    "* CRF model with c1 and c2 penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a . Unclustered data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## In pycrfsuite, A CRF model in can be trained by first creating a \n",
    "## trainer, and then submit the training data and corresponding labels\n",
    "## to the trainer. After that, set the parameters and call train() to \n",
    "## start the training process\n",
    "## CRFSuite: http://www.chokkan.org/software/crfsuite/manual.html#idp8849114176\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=True)\n",
    "\n",
    "# initiate training data to the trainer\n",
    "for x_features, y_labels in zip(X_train, y_train):\n",
    "    trainer.append(x_features, y_labels)\n",
    "\n",
    "# Set the parameters of the model\n",
    "trainer.set_params({\n",
    "    'c1': 0.85,   # coefficient for L1 penalty\n",
    "    'c2': 0.0094,  # coefficient for L2 penalty\n",
    "    # maximum number of iterations\n",
    "    'max_iterations': 1500,\n",
    "\n",
    "    # whether to include transitions that\n",
    "    # are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Clustered data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrainer = pycrfsuite.Trainer(verbose=True)\n",
    "\n",
    "# initiate training data to the trainer\n",
    "for cx_features, cy_labels in zip(cX_train, cy_train):\n",
    "    ctrainer.append(cx_features, cy_labels)\n",
    "\n",
    "# Set the parameters of the model\n",
    "ctrainer.set_params({\n",
    "    'c1': 0.85,   # coefficient for L1 penalty\n",
    "    'c2': 0.0094,  # coefficient for L2 penalty\n",
    "    # maximum number of iterations\n",
    "    'max_iterations': 1500,\n",
    "\n",
    "    # whether to include transitions that\n",
    "    # are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen CRF models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Provide a file name as a parameter to the train function, such that\n",
    "# the model will be saved to the file when training is finished\n",
    "#      'c1': 0.85,\n",
    "#     'c2': 0.0094\n",
    "## clustered claims\n",
    "#1# ctrainer.train(pickle_folder+'crf_KyotoData_version_all_withKmeans_500iter.model')   #  25% test split  # some zero-labels in testsplit\n",
    "#2# ctrainer.train(pickle_folder+'crf_KyotoData_version_all_withKmeans_1000iter.model')      #  30% test split\n",
    "# ctrainer.train(pickle_folder+'crf_KyotoData_version_all_withKmeans_1500iter.model')      #  30% test split\n",
    "# ctrainer.train(pickle_folder+'crf_KyotoData_version_all_withKmeans_100iter_v2.model')      #  30% test split\n",
    "# ctrainer.train(pickle_folder+'crf_KyotoData_version_all_withKmeans_1500iter_v3.model')      #  30% test split\n",
    "\n",
    "\n",
    "## unclustered claims\n",
    "#1# trainer.train(pickle_folder+'crf_KyotoData_version_all_noClus_500iter.model')    #  25% test split\n",
    "#2# trainer.train(pickle_folder+'crf_KyotoData_version_all_noClus_1000iter.model')       #  30% test split\n",
    "#3# trainer.train(pickle_folder+'crf_KyotoData_version_all_noClus_1500iter.model')        #  30% test split\n",
    "# trainer.train(pickle_folder+'crf_KyotoData_version_all_noClus_1500iter_v3.model')        #  30% test split\n",
    "# trainer.train(pickle_folder+'crf_KyotoData_version_all_noClus_1500iter_v4.model')        #  20% test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen test/train splits to display results later with the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freezing test/train for displaying model features later\n",
    "## Clustered\n",
    "#1#\n",
    "# with open(pickle_folder+\"testtrain_withKmeans_500iter.pkl\", 'wb') as f:\n",
    "#     pickle.dump([X_train, X_test, y_train, y_test], f)  \n",
    "#2#\n",
    "# with open(pickle_folder+\"testtrain_withKmeans_1000iter.pkl\", 'wb') as f:\n",
    "#     pickle.dump([X_train, X_test, y_train, y_test], f)  \n",
    "#3#\n",
    "# with open(pickle_folder+\"testtrain_withKmeans_1500iter.pkl\", 'wb') as f:\n",
    "#     pickle.dump([cX_train, cX_test, cy_train, cy_test], f)\n",
    "#4#\n",
    "# with open(pickle_folder+\"testtrain_withKmeans_1500iter_v2.pkl\", 'wb') as f:\n",
    "#     pickle.dump([cX_train, cX_test, cy_train, cy_test], f)\n",
    "#5#\n",
    "# with open(pickle_folder+\"testtrain_withKmeans_100iter_v2.pkl\", 'wb') as f:\n",
    "#     pickle.dump([cX_train, cX_test, cy_train, cy_test], f)\n",
    "#6#\n",
    "# with open(pickle_folder+\"testtrain_withKmeans_1500iter_v3.pkl\", 'wb') as f:\n",
    "#     pickle.dump([cX_train, cX_test, cy_train, cy_test], f)\n",
    "\n",
    "## Unclustered\n",
    "#1#\n",
    "# with open(pickle_folder+\"testtrain_noClus_500iter.pkl\", 'wb') as f:\n",
    "#     pickle.dump([X_train, X_test, y_train, y_test], f)  \n",
    "#2#\n",
    "# with open(pickle_folder+\"testtrain_noClus_1000iter.pkl\", 'wb') as f:\n",
    "#     pickle.dump([X_train, X_test, y_train, y_test], f) \n",
    "# #3#\n",
    "# with open(pickle_folder+\"testtrain_noClus_1500iter.pkl\", 'wb') as f:\n",
    "#     pickle.dump([X_train, X_test, y_train, y_test], f)\n",
    "# #4#\n",
    "# with open(pickle_folder+\"testtrain_noClus_1500iter_v3.pkl\", 'wb') as f:\n",
    "#     pickle.dump([X_train, X_test, y_train, y_test], f)\n",
    "# #5#\n",
    "# with open(pickle_folder+\"testtrain_noClus_1500iter_v4.pkl\", 'wb') as f:\n",
    "#     pickle.dump([X_train, X_test, y_train, y_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_folder+\"testtrain_noClus_1500iter_v3.pkl\", 'rb') as f:\n",
    "    [X_train, X_test, y_train, y_test] = pickle.load(f)\n",
    "with open(pickle_folder+\"testtrain_withKmeans_1500iter_v3.pkl\", 'rb') as f:\n",
    "    [cX_train, cX_test, cy_train, cy_test] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Testing and Evaluating the model\n",
    "* checking the poutput of tagger for random test inputs adn create an evaluation matrix for the classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. CLUSTERED DATA RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once the tagger is done and dusted we test it on the test data\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(pickle_folder+'crf_KyotoData_version_all_withKmeans_1500iter_v3.model')  ## with claim cluster\n",
    "             # pickle_folder+\"crf_KyotoData_version_all_noClus_1500iter.model\"    ## without claim cluster\n",
    "\n",
    "def prediction(X_test):\n",
    "    y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "    return y_pred\n",
    "\n",
    "def tagger_func(i,X_test,y_pred):\n",
    "    for x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n",
    "        print(\"%s \\t(%s)\" % (y, x))\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. \t(O)\n",
      "razali \t(O)\n",
      ", \t(O)\n",
      "who \t(O)\n",
      "was \t(O)\n",
      "a \t(O)\n",
      "major \t(O)\n",
      "player \t(O)\n",
      "in \t(O)\n",
      "the \t(O)\n",
      "rio \t(O)\n",
      "conference \t(O)\n",
      "in \t(O)\n",
      "1992 \t(O)\n",
      ", \t(O)\n",
      "said \t(O)\n",
      "that \t(O)\n",
      "not \t(O)\n",
      "only \t(O)\n",
      "were \t(O)\n",
      "indicators \t(O)\n",
      "of \t(O)\n",
      "environmental \t(O)\n",
      "destruction \t(O)\n",
      "worse \t(O)\n",
      "this \t(O)\n",
      "year \t(O)\n",
      ", \t(O)\n",
      "but \t(O)\n",
      "also \t(O)\n",
      "the \t(O)\n",
      "spirit \t(O)\n",
      "of \t(O)\n",
      "rio \t(O)\n",
      "was \t(O)\n",
      "gone \t(O)\n",
      ". \t(O)\n",
      "''we \t(O)\n",
      "reached \t(O)\n",
      "the \t(O)\n",
      "zenith \t(O)\n",
      "of \t(O)\n",
      "our \t(O)\n",
      "enthusiasm \t(O)\n",
      "and \t(O)\n",
      "commitment \t(O)\n",
      "for \t(O)\n",
      "sustainable \t(O)\n",
      "development \t(O)\n",
      "and \t(O)\n",
      "the \t(O)\n",
      "environment \t(O)\n",
      "in \t(O)\n",
      "1992 \t(O)\n",
      ", \t(O)\n",
      "'' \t(O)\n",
      "he \t(O)\n",
      "said \t(O)\n",
      ". \t(O)\n",
      "'' \t(O)\n"
     ]
    }
   ],
   "source": [
    "# example outputs\n",
    "# Let's take a look at a random sample in the testing set\n",
    "# tagger_func(321,cX_test,cy_pred) # forclus1000\n",
    "# tag_choice = 1\n",
    "cy_pred = prediction(cX_test)\n",
    "tagger_func(131,cX_test,cy_pred) # forclus1000\n",
    "tag_choice = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 13, 'B-ACT': 12, 'I-ACT': 11, 'B-CLAIM-0': 10, 'I-CLAIM-0': 9, 'B-CLAIM-1': 8, 'I-CLAIM-1': 7, 'B-CLAIM-2': 6, 'I-CLAIM-2': 5, 'B-CLAIM-3': 4, 'I-CLAIM-3': 3, 'B-CLAIM-4': 2, 'I-CLAIM-4': 1}\n"
     ]
    }
   ],
   "source": [
    "tag = [actor_claim_tag,new_actor_claim_tag]\n",
    "# dataset for evaluation\n",
    "labels = {}\n",
    "line_count = len(tag[tag_choice])\n",
    "for i in tag[tag_choice]:\n",
    "    labels[i]=line_count\n",
    "    line_count-=1\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "c_predictions = []\n",
    "# predictions = np.array([labels[tag] for row in y_pred for tag in row])\n",
    "for row in cy_pred:\n",
    "    for tag in row:\n",
    "        if(tag == ''):\n",
    "            print('there is empty tag')\n",
    "            tag='O'\n",
    "        c_predictions.append(labels[tag])\n",
    "c_predictions = np.array(c_predictions)\n",
    "# truths = np.array([labels[tag] for row in y_test for tag in row])\n",
    "c_truths = [] \n",
    "for row in cy_test:\n",
    "    for tag in row:\n",
    "        if(tag == ''):\n",
    "            print('there is empty tag')\n",
    "            tag='O'\n",
    "        c_truths.append(labels[tag])\n",
    "c_truths = np.array(c_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   I-CLAIM-4       0.54      0.16      0.25       274\n",
      "   B-CLAIM-4       0.50      0.15      0.23        20\n",
      "   I-CLAIM-3       0.29      0.09      0.14       579\n",
      "   B-CLAIM-3       0.12      0.03      0.04        39\n",
      "   I-CLAIM-2       0.00      0.00      0.00       114\n",
      "   B-CLAIM-2       0.00      0.00      0.00        10\n",
      "   I-CLAIM-1       0.32      0.04      0.07       204\n",
      "   B-CLAIM-1       0.00      0.00      0.00        17\n",
      "   I-CLAIM-0       0.24      0.05      0.09      1311\n",
      "   B-CLAIM-0       0.13      0.02      0.03       117\n",
      "       I-ACT       0.38      0.16      0.22       249\n",
      "       B-ACT       0.45      0.15      0.23       176\n",
      "           O       0.91      0.99      0.95     29125\n",
      "\n",
      "    accuracy                           0.90     32235\n",
      "   macro avg       0.30      0.14      0.17     32235\n",
      "weighted avg       0.85      0.90      0.87     32235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_actor_claim_tag.reverse()\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    c_truths, c_predictions,\n",
    "    target_names= new_actor_claim_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. UNCLUSTERED DATA RESULTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once the tagger is done and dusted we test it on the test data\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(pickle_folder+'crf_KyotoData_version_all_noClus_1500iter_v3.model')  ## with claim cluster\n",
    "             # pickle_folder+\"crf_KyotoData_version_all_noClus_1500iter.model\"    ## without claim cluster\n",
    "\n",
    "def prediction(X_test):\n",
    "    y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "    return y_pred\n",
    "\n",
    "def tagger_func(i,X_test,y_pred):\n",
    "    for x, y in zip(y_pred[i], [x[1].split(\"=\")[1] for x in X_test[i]]):\n",
    "        print(\"%s \\t(%s)\" % (y, x))\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' \t(O)\n",
      "' \t(O)\n",
      "over \t(O)\n"
     ]
    }
   ],
   "source": [
    "# example outputs\n",
    "# tagger_func(282,X_test,y_pred) # forNOTclus1000\n",
    "# tagger_func(244,X_test,y_pred) # forNOTclus1000\n",
    "# tag_choice = 0\n",
    "y_pred = prediction(X_test)\n",
    "tagger_func(59,X_test,y_pred) # forNOTclus1000\n",
    "tag_choice = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 5, 'B-ACT': 4, 'I-ACT': 3, 'B-CLAIM': 2, 'I-CLAIM': 1}\n"
     ]
    }
   ],
   "source": [
    "tag = [actor_claim_tag,new_actor_claim_tag]\n",
    "# dataset for evaluation\n",
    "labels = {}\n",
    "line_count = len(tag[tag_choice])\n",
    "for i in tag[tag_choice]:\n",
    "    labels[i]=line_count\n",
    "    line_count-=1\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is empty tag\n",
      "there is empty tag\n",
      "there is empty tag\n"
     ]
    }
   ],
   "source": [
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = []\n",
    "# predictions = np.array([labels[tag] for row in y_pred for tag in row])\n",
    "for row in y_pred:\n",
    "    for tag in row:\n",
    "        if(tag == ''):\n",
    "            print('there is empty tag')\n",
    "            tag='O'\n",
    "        predictions.append(labels[tag])\n",
    "predictions = np.array(predictions)\n",
    "# truths = np.array([labels[tag] for row in y_test for tag in row])\n",
    "truths = [] \n",
    "for row in y_test:\n",
    "    for tag in row:\n",
    "        if(tag == ''):\n",
    "            print('there is empty tag')\n",
    "            tag='O'\n",
    "        truths.append(labels[tag])\n",
    "truths = np.array(truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     I-CLAIM       0.37      0.15      0.21      2377\n",
      "     B-CLAIM       0.20      0.05      0.08       208\n",
      "       I-ACT       0.33      0.08      0.12       264\n",
      "       B-ACT       0.37      0.08      0.13       170\n",
      "           O       0.92      0.98      0.95     29139\n",
      "\n",
      "    accuracy                           0.90     32158\n",
      "   macro avg       0.44      0.26      0.30     32158\n",
      "weighted avg       0.86      0.90      0.87     32158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actor_claim_tag.reverse()\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    target_names=actor_claim_tag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climachange_env",
   "language": "python",
   "name": "climachange_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
